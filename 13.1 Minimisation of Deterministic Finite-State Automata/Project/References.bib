@incollection{GlobalConvergence,
  title={Nonconvex minimization calculations and the conjugate gradient method},
  author={Powell, Michael JD},
  booktitle={Numerical analysis},
  pages={122--141},
  year={1984},
  publisher={Springer}
}
@misc{Caramanis,
    author = {Caramanis},
    title = {EE 381V: Large Scale Optimization Fall 2012, Lecture 4},
    howpublished = {\url{https://people.cs.umass.edu/~barna/2015-BigData/conv2.pdf}}
}
@article{10.2307/2156398,
 ISSN = {00361429},
 URL = {http://www.jstor.org/stable/2156398},
 abstract = {Conjugate gradient algorithms are used to minimize nonlinear, nonquadratic, real-valued functions on Rn. Rates of convergence are found for several of these algorithms where the conjugate variable is reinitialized every r steps (where r is greater than or equal to n). It is shown in a neighborhood of the minimum that the error, when starting from a point of reinitialization, decreases by order 2 after n steps.},
 author = {Arthur I. Cohen},
 journal = {SIAM Journal on Numerical Analysis},
 number = {2},
 pages = {248--259},
 publisher = {Society for Industrial and Applied Mathematics},
 title = {Rate of Convergence of Several Conjugate Gradient Algorithms},
 volume = {9},
 year = {1972}
}
@article{Powell2Var,
  title={On the convergence of the DFP algorithm for unconstrained optimization when there are only two variables},
  author={Powell, MJD},
  journal={Mathematical programming},
  volume={87},
  number={2},
  pages={281--301},
  year={2000},
  publisher={Springer}
}
	